# ReasonerProxyServer

A lightweight, LLM inference server built using [Sanic](https://sanic.dev/). This framework handles:

- Chat-style prompt parsing  
- Tokenizer-based prompt construction (e.g. LLaMA)  
- Structured input parsing (Triton-style)  
- Clean OpenAI-style output formatting  
- ChatCompletions-compatible HTTP model calls  
- Native response formatting with optional JSON reasoning capture

---

## üöÄ Features

- ‚úÖ Sanic-powered high-performance REST API  
- ‚úÖ Plug-and-play tokenizer support (LLaMA via HuggingFace)  
- ‚úÖ Support for structured input: `inputs`, `outputs`, and `id`  
- ‚úÖ Parses `[BEGIN FINAL RESPONSE] ... [END FINAL RESPONSE]` block

## ‚öôÔ∏è Requirements
```
pip install -r requirements.txt
```

## ‚ñ∂Ô∏è Running the Server
```
python sanic_llm_service.py
```
